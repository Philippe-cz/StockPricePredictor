{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import PullData\n",
    "window_size = 25\n",
    "formation_window = 24\n",
    "target_window = 4\n",
    "\n",
    "GetData = PullData()\n",
    "\n",
    "GetData.fit(ticker = 'nvda',\n",
    "            start_date ='1980-01-01',\n",
    "            end_date = '2022-08-04',\n",
    "            interval = '1wk',\n",
    "            progress = False,\n",
    "            form_window = formation_window,\n",
    "            target_window = target_window,\n",
    "            timeperiod1 = 6,\n",
    "            timeperiod2 = 12,\n",
    "            timeperiod3 = 24,\n",
    "            )\n",
    "\n",
    "data_prep = GetData.transform()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of dataframe:  30025\n",
      "Nr of formations:  1201\n",
      "New length of dataframe:  30025\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from transformers import NormalizeData\n",
    "\n",
    "NormalizeData = NormalizeData()\n",
    "\n",
    "NormalizeData.fit(window_size=25,shuffle=False,debug=False)\n",
    "\n",
    "data_normalized,Dates = NormalizeData.transform(data_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_train window 961.0\n",
      "x_valid window 240.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from training import SplitData\n",
    "\n",
    "SplitData = SplitData()\n",
    "\n",
    "SplitData.fit(split_ratio=0.8, window_size=25,dates=Dates,debug=False)\n",
    "\n",
    "x_train,x_valid,x_train_x, x_valid_x = SplitData.transform(data_normalized)\n",
    " \n",
    "#print(data_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredDataset.fit(window_size=25,batch_size=16,train=True,debug=False)\n",
    "\n",
    "x_train_tensors, _ = GetTensoredDataset.transform(x_train)\n",
    " \n",
    "#print(data_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 24, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredValidDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredValidDataset.fit(window_size=25,batch_size=4,train=False,debug=False)\n",
    "\n",
    "x_valid_tensors, labels = GetTensoredValidDataset.transform(x_valid)\n",
    "x_valid_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_valid.shape[0]/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=6,mode='min', restore_best_weights=True)#\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=10e-15,\n",
    "                              verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss', \n",
    "                             filepath='./nvda_80_model_checkpoint_recent.h5', \n",
    "                             save_best_only=True)\n",
    "\n",
    "callbacks = [early_stopping,reduce_lr,model_checkpoint]\n",
    "\n",
    "def sign_penalty(y_true,y_pred):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true*y_pred,0),\n",
    "                    penalty * tf.square(y_true-y_pred),\n",
    "                    tf.square(y_true - y_pred)\n",
    "                   )\n",
    "    \n",
    "    return(tf.reduce_mean(loss,axis=-1))\n",
    "\n",
    "tf.keras.losses.sign_penalty = sign_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(7788)\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "\n",
    "#     tf.keras.layers.Conv1D(filters=256, kernel_size=10,\n",
    "#                       strides=1, padding=\"same\",\n",
    "#                       activation=tf.nn.selu,\n",
    "#                       input_shape=[None, 7]),\n",
    "# #     tf.keras.layers.Conv1D(filters=512, kernel_size=10,\n",
    "# #                       strides=1, padding=\"same\",\n",
    "# #                       activation=tf.nn.selu,\n",
    "# #                       input_shape=[None, 7]),\n",
    "    \n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     # tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "#     #tf.keras.layers.Dropout(0.2),   \n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),   \n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(4, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(3, activation=tf.nn.selu), #4\n",
    "#     tf.keras.layers.Dense(1,activation=tf.nn.relu),\n",
    "# ])\n",
    "\n",
    "# #optimizer1 = tf.keras.optimizers.SGD(learning_rate=10e-7, momentum=0.9)\n",
    "# #optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# #optimizer3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "# #optimizer4 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta')\n",
    "\n",
    "# optimizer5 = tf.keras.optimizers.Adagrad(learning_rate=0.0001, initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "\n",
    "# model.compile(loss=sign_penalty,\n",
    "#               optimizer=optimizer5,\n",
    "#               )\n",
    "\n",
    "# model.fit(train_set, epochs=120,callbacks=[callbacks],validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "61/61 [==============================] - 8s 54ms/step - loss: 0.0369 - val_loss: 0.0280 - lr: 0.0100\n",
      "Epoch 2/6\n",
      "61/61 [==============================] - 1s 18ms/step - loss: 0.0292 - val_loss: 0.0260 - lr: 0.0100\n",
      "Epoch 3/6\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0272 - val_loss: 0.0247 - lr: 0.0100\n",
      "Epoch 4/6\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0259 - val_loss: 0.0238 - lr: 0.0100\n",
      "Epoch 5/6\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.0248 - val_loss: 0.0228 - lr: 0.0100\n",
      "Epoch 6/6\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0239 - val_loss: 0.0221 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206cbbc5f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(7788)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=1,\n",
    "                      strides=1, padding=\"same\",\n",
    "                      activation=tf.nn.selu,\n",
    "                      input_shape=[None, 7]),\n",
    "     tf.keras.layers.Conv1D(filters=16, kernel_size=1,\n",
    "                      strides=1, padding=\"same\",\n",
    "                      activation=tf.nn.elu,\n",
    "                      #input_shape=[None, 7]\n",
    "                           ),\n",
    "         tf.keras.layers.Conv1D(filters=32, kernel_size=10,\n",
    "                      strides=1, padding=\"same\",\n",
    "                      activation=tf.nn.selu,\n",
    "                      #input_shape=[None, 7]\n",
    "                           ),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(3, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(3)),\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(1,activation=tf.nn.relu),\n",
    "    ])\n",
    "\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "optimizer5 = tf.keras.optimizers.Adagrad(learning_rate=0.01, initial_accumulator_value=4, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "model.compile(loss=sign_penalty,\n",
    "              optimizer=optimizer5,\n",
    "              )\n",
    "\n",
    "model.fit(x_train_tensors, epochs=6,callbacks=[callbacks],validation_data=x_valid_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95655805, 0.91791713], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forecast(model, series, window_size,debug):\n",
    "\n",
    "    \"\"\"\n",
    "    Get model, data and window size as an input. \n",
    "    Make prediction window is subtracted by 1, since we do not need label in window, \n",
    "    label value is skipped\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size-1, shift=window_size, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    \n",
    "    if debug == True:\n",
    "        #This block of code will print out data on which is made prediction\n",
    "        for item in ds:\n",
    "            c += 1\n",
    "            if c <3:\n",
    "                print(\"\\n\"+str(c)+ \" prediction:\\n \",item)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    ds = ds.batch(1).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    forecast2 = np.squeeze(forecast)\n",
    "    return forecast2\n",
    "\n",
    "forecast = model_forecast(model, x_valid, window_size=window_size,debug=False)\n",
    "forecast[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Raw prediction:  [[0.95655805]]\n"
     ]
    }
   ],
   "source": [
    "pr = x_valid.iloc[:24,:].to_numpy()\n",
    "pr = np.array([pr])\n",
    "pr = np.array([pr])\n",
    "pred = tf.data.Dataset.from_tensor_slices(pr)\n",
    "predict = model.predict(pred)\n",
    "print(\"Raw prediction: \",predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>EMA6</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA24</th>\n",
       "      <th>labels</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.352501</td>\n",
       "      <td>40.435001</td>\n",
       "      <td>38.305000</td>\n",
       "      <td>38.457500</td>\n",
       "      <td>36.011488</td>\n",
       "      <td>33.162876</td>\n",
       "      <td>29.691369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.790001</td>\n",
       "      <td>39.150002</td>\n",
       "      <td>35.875000</td>\n",
       "      <td>36.139999</td>\n",
       "      <td>36.048206</td>\n",
       "      <td>33.620895</td>\n",
       "      <td>30.207260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.262501</td>\n",
       "      <td>36.875000</td>\n",
       "      <td>34.645000</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>36.231575</td>\n",
       "      <td>34.093064</td>\n",
       "      <td>30.725879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.435001</td>\n",
       "      <td>41.575001</td>\n",
       "      <td>37.169998</td>\n",
       "      <td>41.237499</td>\n",
       "      <td>37.661839</td>\n",
       "      <td>35.192208</td>\n",
       "      <td>31.566809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.582500</td>\n",
       "      <td>42.325001</td>\n",
       "      <td>40.325001</td>\n",
       "      <td>42.025002</td>\n",
       "      <td>38.908457</td>\n",
       "      <td>36.243407</td>\n",
       "      <td>32.403464</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>191.389999</td>\n",
       "      <td>193.369995</td>\n",
       "      <td>168.690002</td>\n",
       "      <td>169.740005</td>\n",
       "      <td>183.115982</td>\n",
       "      <td>197.401152</td>\n",
       "      <td>214.717334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>168.679993</td>\n",
       "      <td>153.279999</td>\n",
       "      <td>158.800003</td>\n",
       "      <td>176.168560</td>\n",
       "      <td>191.462513</td>\n",
       "      <td>210.243947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>164.750000</td>\n",
       "      <td>171.399994</td>\n",
       "      <td>158.529999</td>\n",
       "      <td>171.259995</td>\n",
       "      <td>174.766113</td>\n",
       "      <td>188.354434</td>\n",
       "      <td>207.125231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>173.119995</td>\n",
       "      <td>173.300003</td>\n",
       "      <td>143.919998</td>\n",
       "      <td>145.229996</td>\n",
       "      <td>166.327222</td>\n",
       "      <td>181.719905</td>\n",
       "      <td>202.173612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>141.750000</td>\n",
       "      <td>182.440002</td>\n",
       "      <td>140.550003</td>\n",
       "      <td>181.630005</td>\n",
       "      <td>166.327222</td>\n",
       "      <td>181.719905</td>\n",
       "      <td>202.173612</td>\n",
       "      <td>182.440002</td>\n",
       "      <td>215.191139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low       Close        EMA6       EMA12  \\\n",
       "In                                                                             \n",
       "0      38.352501   40.435001   38.305000   38.457500   36.011488   33.162876   \n",
       "1      38.790001   39.150002   35.875000   36.139999   36.048206   33.620895   \n",
       "2      36.262501   36.875000   34.645000   36.689999   36.231575   34.093064   \n",
       "3      37.435001   41.575001   37.169998   41.237499   37.661839   35.192208   \n",
       "4      41.582500   42.325001   40.325001   42.025002   38.908457   36.243407   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "5995  191.389999  193.369995  168.690002  169.740005  183.115982  197.401152   \n",
       "5996  160.000000  168.679993  153.279999  158.800003  176.168560  191.462513   \n",
       "5997  164.750000  171.399994  158.529999  171.259995  174.766113  188.354434   \n",
       "5998  173.119995  173.300003  143.919998  145.229996  166.327222  181.719905   \n",
       "5999  141.750000  182.440002  140.550003  181.630005  166.327222  181.719905   \n",
       "\n",
       "           EMA24      labels  prediction  \n",
       "In                                        \n",
       "0      29.691369         NaN         NaN  \n",
       "1      30.207260         NaN         NaN  \n",
       "2      30.725879         NaN         NaN  \n",
       "3      31.566809         NaN         NaN  \n",
       "4      32.403464         NaN         NaN  \n",
       "...          ...         ...         ...  \n",
       "5995  214.717334         NaN         NaN  \n",
       "5996  210.243947         NaN         NaN  \n",
       "5997  207.125231         NaN         NaN  \n",
       "5998  202.173612         NaN         NaN  \n",
       "5999  202.173612  182.440002  215.191139  \n",
       "\n",
       "[6000 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ReverseNormalization\n",
    "\n",
    "ReverseNormalization = ReverseNormalization()\n",
    "\n",
    "ReverseNormalization.fit(forecasts=forecast,labels=labels,x_valid = x_valid,x_valid_x = x_valid_x,window_size=25,debug=False)\n",
    "\n",
    "df = ReverseNormalization.transform()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('04_stockprediction': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ae9b00e5548af57dc5d4c583df0ad518b3d501960f1be5c69ca4a560e00ae05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
