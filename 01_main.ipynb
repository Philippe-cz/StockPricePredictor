{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import PullData\n",
    "\n",
    "window_size = 25\n",
    "formation_window = 24\n",
    "target_window = 4\n",
    "\n",
    "GetData = PullData()\n",
    "\n",
    "GetData.fit(ticker='nvda',\n",
    "            start_date='1980-01-01',\n",
    "            end_date='2022-08-04',\n",
    "            interval='1wk',\n",
    "            progress=False,\n",
    "            condition=False,\n",
    "            form_window=formation_window,\n",
    "            target_window=target_window,\n",
    "            timeperiod1=6,\n",
    "            timeperiod2=12,\n",
    "            timeperiod3=24,\n",
    "            export_excel=True\n",
    "            )\n",
    "\n",
    "data_prep = GetData.transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of dataframe:  30025\n",
      "Nr of formations:  1201\n",
      "New length of dataframe:  30025\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from transformers import NormalizeData\n",
    "\n",
    "NormalizeData = NormalizeData()\n",
    "\n",
    "NormalizeData.fit(window_size=25, shuffle=False, debug=False)\n",
    "\n",
    "data_normalized, Dates = NormalizeData.transform(data_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:   30025\n",
      "total windows in dataset:  1201.0\n",
      "\n",
      "total windows of 80.0% train set: 961.0 \n",
      "total windows of 20% valid set: 240.0 \n",
      "\n",
      "x_train window 961.0\n",
      "x_valid window 240.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from training import SplitData\n",
    "\n",
    "SplitData = SplitData()\n",
    "\n",
    "SplitData.fit(split_ratio=0.8, window_size=25, dates=Dates, debug=True)\n",
    "\n",
    "x_train, x_valid, x_train_x, x_valid_x = SplitData.transform(data_normalized)\n",
    "\n",
    "# print(data_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, None), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredDataset.fit(window_size=25, batch_size=16, train=True, debug=False)\n",
    "\n",
    "x_train_tensors, _ = GetTensoredDataset.transform(x_train)\n",
    "\n",
    "x_train_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 24, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training import GetTensoredDataset\n",
    "\n",
    "GetTensoredValidDataset = GetTensoredDataset()\n",
    "\n",
    "GetTensoredValidDataset.fit(\n",
    "    window_size=25, batch_size=4, train=False, debug=False)\n",
    "\n",
    "x_valid_tensors, labels = GetTensoredValidDataset.transform(x_valid)\n",
    "x_valid_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=10e-15,\n",
    "                              verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss',\n",
    "                                   filepath='./nvda_80_model_checkpoint_recent.h5',\n",
    "                                   save_best_only=True)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
    "\n",
    "\n",
    "def sign_penalty(y_true, y_pred):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true*y_pred, 0),\n",
    "                    penalty * tf.square(y_true-y_pred),\n",
    "                    tf.square(y_true - y_pred)\n",
    "                    )\n",
    "\n",
    "    return(tf.reduce_mean(loss, axis=-1))\n",
    "\n",
    "\n",
    "tf.keras.losses.sign_penalty = sign_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(7788)\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "\n",
    "#     tf.keras.layers.Conv1D(filters=256, kernel_size=10,\n",
    "#                       strides=1, padding=\"same\",\n",
    "#                       activation=tf.nn.selu,\n",
    "#                       input_shape=[None, 7]),\n",
    "# #     tf.keras.layers.Conv1D(filters=512, kernel_size=10,\n",
    "# #                       strides=1, padding=\"same\",\n",
    "# #                       activation=tf.nn.selu,\n",
    "# #                       input_shape=[None, 7]),\n",
    "\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     # tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(1024, activation=tf.nn.selu),\n",
    "#     #tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(16, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(4, activation=tf.nn.selu),\n",
    "#     tf.keras.layers.Dense(3, activation=tf.nn.selu), #4\n",
    "#     tf.keras.layers.Dense(1,activation=tf.nn.relu),\n",
    "# ])\n",
    "\n",
    "# #optimizer1 = tf.keras.optimizers.SGD(learning_rate=10e-7, momentum=0.9)\n",
    "# #optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "# #optimizer3 = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "# #optimizer4 = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta')\n",
    "\n",
    "# optimizer5 = tf.keras.optimizers.Adagrad(learning_rate=0.0001, initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "\n",
    "# model.compile(loss=sign_penalty,\n",
    "#               optimizer=optimizer5,\n",
    "#               )\n",
    "\n",
    "# model.fit(train_set, epochs=120,callbacks=[callbacks],validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "61/61 [==============================] - 8s 63ms/step - loss: 0.0369 - val_loss: 0.0280 - lr: 0.0100\n",
      "Epoch 2/120\n",
      "61/61 [==============================] - 1s 17ms/step - loss: 0.0292 - val_loss: 0.0260 - lr: 0.0100\n",
      "Epoch 3/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0272 - val_loss: 0.0247 - lr: 0.0100\n",
      "Epoch 4/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0259 - val_loss: 0.0238 - lr: 0.0100\n",
      "Epoch 5/120\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 0.0248 - val_loss: 0.0228 - lr: 0.0100\n",
      "Epoch 6/120\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 0.0239 - val_loss: 0.0221 - lr: 0.0100\n",
      "Epoch 7/120\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 0.0232 - val_loss: 0.0218 - lr: 0.0100\n",
      "Epoch 8/120\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.0226 - val_loss: 0.0212 - lr: 0.0100\n",
      "Epoch 9/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0221 - val_loss: 0.0209 - lr: 0.0100\n",
      "Epoch 10/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0217 - val_loss: 0.0205 - lr: 0.0100\n",
      "Epoch 11/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0214 - val_loss: 0.0203 - lr: 0.0100\n",
      "Epoch 12/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0211 - val_loss: 0.0202 - lr: 0.0100\n",
      "Epoch 13/120\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.0209 - val_loss: 0.0199 - lr: 0.0100\n",
      "Epoch 14/120\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 0.0207 - val_loss: 0.0197 - lr: 0.0100\n",
      "Epoch 15/120\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 0.0205 - val_loss: 0.0195 - lr: 0.0100\n",
      "Epoch 16/120\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.0203 - val_loss: 0.0193 - lr: 0.0100\n",
      "Epoch 17/120\n",
      "61/61 [==============================] - 2s 27ms/step - loss: 0.0201 - val_loss: 0.0191 - lr: 0.0100\n",
      "Epoch 18/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0200 - val_loss: 0.0190 - lr: 0.0100\n",
      "Epoch 19/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0199 - val_loss: 0.0188 - lr: 0.0100\n",
      "Epoch 20/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0198 - val_loss: 0.0187 - lr: 0.0100\n",
      "Epoch 21/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0197 - val_loss: 0.0186 - lr: 0.0100\n",
      "Epoch 22/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0196 - val_loss: 0.0185 - lr: 0.0100\n",
      "Epoch 23/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0195 - val_loss: 0.0183 - lr: 0.0100\n",
      "Epoch 24/120\n",
      "61/61 [==============================] - 2s 25ms/step - loss: 0.0194 - val_loss: 0.0182 - lr: 0.0100\n",
      "Epoch 25/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0193 - val_loss: 0.0181 - lr: 0.0100\n",
      "Epoch 26/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0193 - val_loss: 0.0180 - lr: 0.0100\n",
      "Epoch 27/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0192 - val_loss: 0.0179 - lr: 0.0100\n",
      "Epoch 28/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0191 - val_loss: 0.0178 - lr: 0.0100\n",
      "Epoch 29/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0191 - val_loss: 0.0177 - lr: 0.0100\n",
      "Epoch 30/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0190 - val_loss: 0.0176 - lr: 0.0100\n",
      "Epoch 31/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0189 - val_loss: 0.0175 - lr: 0.0100\n",
      "Epoch 32/120\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0189 - val_loss: 0.0174 - lr: 0.0100\n",
      "Epoch 33/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0188 - val_loss: 0.0173 - lr: 0.0100\n",
      "Epoch 34/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0188 - val_loss: 0.0172 - lr: 0.0100\n",
      "Epoch 35/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0187 - val_loss: 0.0171 - lr: 0.0100\n",
      "Epoch 36/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0187 - val_loss: 0.0170 - lr: 0.0100\n",
      "Epoch 37/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0186 - val_loss: 0.0170 - lr: 0.0100\n",
      "Epoch 38/120\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.0186 - val_loss: 0.0169 - lr: 0.0100\n",
      "Epoch 39/120\n",
      "61/61 [==============================] - 2s 26ms/step - loss: 0.0185 - val_loss: 0.0168 - lr: 0.0100\n",
      "Epoch 40/120\n",
      "61/61 [==============================] - 2s 28ms/step - loss: 0.0185 - val_loss: 0.0167 - lr: 0.0100\n",
      "Epoch 41/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0184 - val_loss: 0.0166 - lr: 0.0100\n",
      "Epoch 42/120\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0184 - val_loss: 0.0166 - lr: 0.0100\n",
      "Epoch 43/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0183 - val_loss: 0.0165 - lr: 0.0100\n",
      "Epoch 44/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0183 - val_loss: 0.0165 - lr: 0.0100\n",
      "Epoch 45/120\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0182\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0182 - val_loss: 0.0164 - lr: 0.0100\n",
      "Epoch 46/120\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 0.0198 - val_loss: 0.0150 - lr: 0.0020\n",
      "Epoch 47/120\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 0.0184 - val_loss: 0.0152 - lr: 0.0020\n",
      "Epoch 48/120\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0183\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0183 - val_loss: 0.0152 - lr: 0.0020\n",
      "Epoch 49/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0181 - val_loss: 0.0154 - lr: 4.0000e-04\n",
      "Epoch 50/120\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0181\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "61/61 [==============================] - 1s 23ms/step - loss: 0.0181 - val_loss: 0.0155 - lr: 4.0000e-04\n",
      "Epoch 51/120\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 0.0180 - val_loss: 0.0155 - lr: 8.0000e-05\n",
      "Epoch 52/120\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.0182\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 0.0180 - val_loss: 0.0155 - lr: 8.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cac03dd040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(7788)\n",
    "np.random.seed(7788)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=8, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           input_shape=[None, 7]),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=1,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.elu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=10,\n",
    "                           strides=1, padding=\"same\",\n",
    "                           activation=tf.nn.selu,\n",
    "                           #input_shape=[None, 7]\n",
    "                           ),\n",
    "    #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(3, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(3)),\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.selu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.relu),\n",
    "])\n",
    "\n",
    "optimizer2 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "optimizer5 = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.01, initial_accumulator_value=4, epsilon=1e-07, name='Adagrad')\n",
    "\n",
    "model.compile(loss=sign_penalty,\n",
    "              optimizer=optimizer5,\n",
    "              )\n",
    "\n",
    "model.fit(x_train_tensors, epochs=120, callbacks=[\n",
    "          callbacks], validation_data=x_valid_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9225807 , 0.86792785], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forecast(model, series, window_size, debug):\n",
    "    \"\"\"\n",
    "    Get model, data and window size as an input. \n",
    "    Make prediction window is subtracted by 1, since we do not need label in window, \n",
    "    label value is skipped\n",
    "    \"\"\"\n",
    "    c = 0\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size-1, shift=window_size, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "\n",
    "    if debug == True:\n",
    "        # This block of code will print out data on which is made prediction\n",
    "        for item in ds:\n",
    "            c += 1\n",
    "            if c < 3:\n",
    "                print(\"\\n\"+str(c) + \" prediction:\\n \", item)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    ds = ds.batch(1).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    forecast2 = np.squeeze(forecast)\n",
    "    return forecast2\n",
    "\n",
    "\n",
    "forecast = model_forecast(model, x_valid, window_size=window_size, debug=False)\n",
    "forecast[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "Raw prediction:  [[0.9225807]]\n"
     ]
    }
   ],
   "source": [
    "pr = x_valid.iloc[:24, :].to_numpy()\n",
    "pr = np.array([pr])\n",
    "pr = np.array([pr])\n",
    "pred = tf.data.Dataset.from_tensor_slices(pr)\n",
    "predict = model.predict(pred)\n",
    "print(\"Raw prediction: \", predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6000, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ReverseNormalization\n",
    "\n",
    "ReverseNormalization = ReverseNormalization()\n",
    "\n",
    "ReverseNormalization.fit(forecasts=forecast, labels=labels,\n",
    "                         x_valid=x_valid, x_valid_x=x_valid_x, window_size=25, debug=False)\n",
    "\n",
    "df = ReverseNormalization.transform()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from final_evaluation import GetFinalDataframe\n",
    "\n",
    "GetFinalDataframe = GetFinalDataframe()\n",
    "\n",
    "GetFinalDataframe.fit(dates=Dates,\n",
    "                      x_valid=x_valid)\n",
    "\n",
    "df1 = GetFinalDataframe.transform(df)\n",
    "df1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry Candle:  Current Open\n",
      "\n",
      "Total Trades:  232\n",
      "Profit Trades:  178\n",
      "Loss Trades:  54\n",
      "\n",
      "Win Ratio: 77.0 %\n",
      "Loss Ratio: 23 %\n",
      "\n",
      "Average profit per trade:  767\n",
      "\n",
      "Gross profit:  177872\n",
      "Gross loss:  -54734\n",
      "\n",
      "Net profit:  123138\n"
     ]
    }
   ],
   "source": [
    "from final_evaluation import GetModelPerformance\n",
    "\n",
    "GetModelPerformance = GetModelPerformance()\n",
    "\n",
    "GetModelPerformance.fit(acceptance=0,\n",
    "                        penalization=0,\n",
    "                        entry_candle='Current Open',\n",
    "                        budget=10000,\n",
    "                        export_excel=True)\n",
    "\n",
    "trades_df = GetModelPerformance.transform(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>EMA6</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA24</th>\n",
       "      <th>labels</th>\n",
       "      <th>prediction</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>profit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.352501</td>\n",
       "      <td>40.435001</td>\n",
       "      <td>38.305000</td>\n",
       "      <td>38.457500</td>\n",
       "      <td>36.011488</td>\n",
       "      <td>33.162876</td>\n",
       "      <td>29.691369</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.790001</td>\n",
       "      <td>39.150002</td>\n",
       "      <td>35.875000</td>\n",
       "      <td>36.139999</td>\n",
       "      <td>36.048206</td>\n",
       "      <td>33.620895</td>\n",
       "      <td>30.207260</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-06-26</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.262501</td>\n",
       "      <td>36.875000</td>\n",
       "      <td>34.645000</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>36.231575</td>\n",
       "      <td>34.093064</td>\n",
       "      <td>30.725879</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.435001</td>\n",
       "      <td>41.575001</td>\n",
       "      <td>37.169998</td>\n",
       "      <td>41.237499</td>\n",
       "      <td>37.661839</td>\n",
       "      <td>35.192208</td>\n",
       "      <td>31.566809</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.582500</td>\n",
       "      <td>42.325001</td>\n",
       "      <td>40.325001</td>\n",
       "      <td>42.025002</td>\n",
       "      <td>38.908457</td>\n",
       "      <td>36.243407</td>\n",
       "      <td>32.403464</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.097500</td>\n",
       "      <td>42.482498</td>\n",
       "      <td>39.389999</td>\n",
       "      <td>41.097500</td>\n",
       "      <td>39.533898</td>\n",
       "      <td>36.990191</td>\n",
       "      <td>33.098987</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.235001</td>\n",
       "      <td>42.517502</td>\n",
       "      <td>40.154999</td>\n",
       "      <td>41.802502</td>\n",
       "      <td>40.182070</td>\n",
       "      <td>37.730546</td>\n",
       "      <td>33.795268</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42.097500</td>\n",
       "      <td>43.639999</td>\n",
       "      <td>38.227501</td>\n",
       "      <td>38.990002</td>\n",
       "      <td>39.841479</td>\n",
       "      <td>37.924309</td>\n",
       "      <td>34.210847</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39.917500</td>\n",
       "      <td>42.417500</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>40.375000</td>\n",
       "      <td>39.993914</td>\n",
       "      <td>38.301338</td>\n",
       "      <td>34.703979</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40.622501</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>39.342499</td>\n",
       "      <td>40.952499</td>\n",
       "      <td>40.267795</td>\n",
       "      <td>38.709209</td>\n",
       "      <td>35.203861</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41.145000</td>\n",
       "      <td>42.974998</td>\n",
       "      <td>40.564999</td>\n",
       "      <td>42.615002</td>\n",
       "      <td>40.938426</td>\n",
       "      <td>39.310100</td>\n",
       "      <td>35.796752</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42.095001</td>\n",
       "      <td>42.145000</td>\n",
       "      <td>40.677502</td>\n",
       "      <td>40.922501</td>\n",
       "      <td>40.933876</td>\n",
       "      <td>39.558162</td>\n",
       "      <td>36.206812</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>41.384998</td>\n",
       "      <td>45.027500</td>\n",
       "      <td>41.330002</td>\n",
       "      <td>45.027500</td>\n",
       "      <td>42.103483</td>\n",
       "      <td>40.399598</td>\n",
       "      <td>36.912467</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-11</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>46.285000</td>\n",
       "      <td>47.799999</td>\n",
       "      <td>44.402500</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>42.859630</td>\n",
       "      <td>41.068891</td>\n",
       "      <td>37.539470</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-18</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.932499</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>44.692501</td>\n",
       "      <td>43.383308</td>\n",
       "      <td>41.626369</td>\n",
       "      <td>38.111712</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-25</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45.200001</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>45.325001</td>\n",
       "      <td>43.938077</td>\n",
       "      <td>42.195390</td>\n",
       "      <td>38.688775</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45.697498</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>45.507500</td>\n",
       "      <td>48.647499</td>\n",
       "      <td>45.283626</td>\n",
       "      <td>43.188022</td>\n",
       "      <td>39.485473</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-09</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.950001</td>\n",
       "      <td>49.897499</td>\n",
       "      <td>48.112499</td>\n",
       "      <td>49.224998</td>\n",
       "      <td>46.409733</td>\n",
       "      <td>44.116788</td>\n",
       "      <td>40.264635</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>49.445000</td>\n",
       "      <td>50.467499</td>\n",
       "      <td>47.792500</td>\n",
       "      <td>50.465000</td>\n",
       "      <td>47.568381</td>\n",
       "      <td>45.093436</td>\n",
       "      <td>41.080664</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.465000</td>\n",
       "      <td>52.492500</td>\n",
       "      <td>50.305000</td>\n",
       "      <td>52.172501</td>\n",
       "      <td>48.883843</td>\n",
       "      <td>46.182523</td>\n",
       "      <td>41.968011</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-30</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51.799999</td>\n",
       "      <td>54.667500</td>\n",
       "      <td>50.092499</td>\n",
       "      <td>54.035000</td>\n",
       "      <td>50.355602</td>\n",
       "      <td>47.390596</td>\n",
       "      <td>42.933370</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54.035000</td>\n",
       "      <td>54.292500</td>\n",
       "      <td>51.950001</td>\n",
       "      <td>52.840000</td>\n",
       "      <td>51.065430</td>\n",
       "      <td>48.228966</td>\n",
       "      <td>43.725901</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>52.997501</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>52.625000</td>\n",
       "      <td>54.240002</td>\n",
       "      <td>51.972451</td>\n",
       "      <td>49.153741</td>\n",
       "      <td>44.567029</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54.327499</td>\n",
       "      <td>54.340000</td>\n",
       "      <td>47.807499</td>\n",
       "      <td>49.419998</td>\n",
       "      <td>51.243179</td>\n",
       "      <td>49.194703</td>\n",
       "      <td>44.955266</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.012501</td>\n",
       "      <td>50.075001</td>\n",
       "      <td>45.145000</td>\n",
       "      <td>48.375000</td>\n",
       "      <td>36.048206</td>\n",
       "      <td>33.620895</td>\n",
       "      <td>30.207260</td>\n",
       "      <td>50.075001</td>\n",
       "      <td>52.733866</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>-1.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.790001</td>\n",
       "      <td>39.150002</td>\n",
       "      <td>35.875000</td>\n",
       "      <td>36.139999</td>\n",
       "      <td>36.048206</td>\n",
       "      <td>33.620895</td>\n",
       "      <td>30.207260</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-06-26</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>36.262501</td>\n",
       "      <td>36.875000</td>\n",
       "      <td>34.645000</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>36.231575</td>\n",
       "      <td>34.093064</td>\n",
       "      <td>30.725879</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37.435001</td>\n",
       "      <td>41.575001</td>\n",
       "      <td>37.169998</td>\n",
       "      <td>41.237499</td>\n",
       "      <td>37.661839</td>\n",
       "      <td>35.192208</td>\n",
       "      <td>31.566809</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41.582500</td>\n",
       "      <td>42.325001</td>\n",
       "      <td>40.325001</td>\n",
       "      <td>42.025002</td>\n",
       "      <td>38.908457</td>\n",
       "      <td>36.243407</td>\n",
       "      <td>32.403464</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42.097500</td>\n",
       "      <td>42.482498</td>\n",
       "      <td>39.389999</td>\n",
       "      <td>41.097500</td>\n",
       "      <td>39.533898</td>\n",
       "      <td>36.990191</td>\n",
       "      <td>33.098987</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>41.235001</td>\n",
       "      <td>42.517502</td>\n",
       "      <td>40.154999</td>\n",
       "      <td>41.802502</td>\n",
       "      <td>40.182070</td>\n",
       "      <td>37.730546</td>\n",
       "      <td>33.795268</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>42.097500</td>\n",
       "      <td>43.639999</td>\n",
       "      <td>38.227501</td>\n",
       "      <td>38.990002</td>\n",
       "      <td>39.841479</td>\n",
       "      <td>37.924309</td>\n",
       "      <td>34.210847</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>39.917500</td>\n",
       "      <td>42.417500</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>40.375000</td>\n",
       "      <td>39.993914</td>\n",
       "      <td>38.301338</td>\n",
       "      <td>34.703979</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40.622501</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>39.342499</td>\n",
       "      <td>40.952499</td>\n",
       "      <td>40.267795</td>\n",
       "      <td>38.709209</td>\n",
       "      <td>35.203861</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>41.145000</td>\n",
       "      <td>42.974998</td>\n",
       "      <td>40.564999</td>\n",
       "      <td>42.615002</td>\n",
       "      <td>40.938426</td>\n",
       "      <td>39.310100</td>\n",
       "      <td>35.796752</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>42.095001</td>\n",
       "      <td>42.145000</td>\n",
       "      <td>40.677502</td>\n",
       "      <td>40.922501</td>\n",
       "      <td>40.933876</td>\n",
       "      <td>39.558162</td>\n",
       "      <td>36.206812</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41.384998</td>\n",
       "      <td>45.027500</td>\n",
       "      <td>41.330002</td>\n",
       "      <td>45.027500</td>\n",
       "      <td>42.103483</td>\n",
       "      <td>40.399598</td>\n",
       "      <td>36.912467</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-11</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>46.285000</td>\n",
       "      <td>47.799999</td>\n",
       "      <td>44.402500</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>42.859630</td>\n",
       "      <td>41.068891</td>\n",
       "      <td>37.539470</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-18</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44.437500</td>\n",
       "      <td>44.932499</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>44.692501</td>\n",
       "      <td>43.383308</td>\n",
       "      <td>41.626369</td>\n",
       "      <td>38.111712</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-09-25</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45.200001</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>45.325001</td>\n",
       "      <td>43.938077</td>\n",
       "      <td>42.195390</td>\n",
       "      <td>38.688775</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>45.697498</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>45.507500</td>\n",
       "      <td>48.647499</td>\n",
       "      <td>45.283626</td>\n",
       "      <td>43.188022</td>\n",
       "      <td>39.485473</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-09</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>48.950001</td>\n",
       "      <td>49.897499</td>\n",
       "      <td>48.112499</td>\n",
       "      <td>49.224998</td>\n",
       "      <td>46.409733</td>\n",
       "      <td>44.116788</td>\n",
       "      <td>40.264635</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>49.445000</td>\n",
       "      <td>50.467499</td>\n",
       "      <td>47.792500</td>\n",
       "      <td>50.465000</td>\n",
       "      <td>47.568381</td>\n",
       "      <td>45.093436</td>\n",
       "      <td>41.080664</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.465000</td>\n",
       "      <td>52.492500</td>\n",
       "      <td>50.305000</td>\n",
       "      <td>52.172501</td>\n",
       "      <td>48.883843</td>\n",
       "      <td>46.182523</td>\n",
       "      <td>41.968011</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-10-30</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>51.799999</td>\n",
       "      <td>54.667500</td>\n",
       "      <td>50.092499</td>\n",
       "      <td>54.035000</td>\n",
       "      <td>50.355602</td>\n",
       "      <td>47.390596</td>\n",
       "      <td>42.933370</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>54.035000</td>\n",
       "      <td>54.292500</td>\n",
       "      <td>51.950001</td>\n",
       "      <td>52.840000</td>\n",
       "      <td>51.065430</td>\n",
       "      <td>48.228966</td>\n",
       "      <td>43.725901</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>52.997501</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>52.625000</td>\n",
       "      <td>54.240002</td>\n",
       "      <td>51.972451</td>\n",
       "      <td>49.153741</td>\n",
       "      <td>44.567029</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>54.327499</td>\n",
       "      <td>54.340000</td>\n",
       "      <td>47.807499</td>\n",
       "      <td>49.419998</td>\n",
       "      <td>51.243179</td>\n",
       "      <td>49.194703</td>\n",
       "      <td>44.955266</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50.012501</td>\n",
       "      <td>50.075001</td>\n",
       "      <td>45.145000</td>\n",
       "      <td>47.872501</td>\n",
       "      <td>50.280128</td>\n",
       "      <td>48.991288</td>\n",
       "      <td>45.188645</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>48.014999</td>\n",
       "      <td>54.512501</td>\n",
       "      <td>46.150002</td>\n",
       "      <td>53.849998</td>\n",
       "      <td>36.231575</td>\n",
       "      <td>34.093064</td>\n",
       "      <td>30.725879</td>\n",
       "      <td>54.512501</td>\n",
       "      <td>51.436983</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>3.421984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open       High        Low      Close       EMA6      EMA12  \\\n",
       "In                                                                     \n",
       "0   38.352501  40.435001  38.305000  38.457500  36.011488  33.162876   \n",
       "1   38.790001  39.150002  35.875000  36.139999  36.048206  33.620895   \n",
       "2   36.262501  36.875000  34.645000  36.689999  36.231575  34.093064   \n",
       "3   37.435001  41.575001  37.169998  41.237499  37.661839  35.192208   \n",
       "4   41.582500  42.325001  40.325001  42.025002  38.908457  36.243407   \n",
       "5   42.097500  42.482498  39.389999  41.097500  39.533898  36.990191   \n",
       "6   41.235001  42.517502  40.154999  41.802502  40.182070  37.730546   \n",
       "7   42.097500  43.639999  38.227501  38.990002  39.841479  37.924309   \n",
       "8   39.917500  42.417500  39.799999  40.375000  39.993914  38.301338   \n",
       "9   40.622501  41.820000  39.342499  40.952499  40.267795  38.709209   \n",
       "10  41.145000  42.974998  40.564999  42.615002  40.938426  39.310100   \n",
       "11  42.095001  42.145000  40.677502  40.922501  40.933876  39.558162   \n",
       "12  41.384998  45.027500  41.330002  45.027500  42.103483  40.399598   \n",
       "13  46.285000  47.799999  44.402500  44.750000  42.859630  41.068891   \n",
       "14  44.437500  44.932499  42.540001  44.692501  43.383308  41.626369   \n",
       "15  45.200001  45.500000  44.250000  45.325001  43.938077  42.195390   \n",
       "16  45.697498  48.750000  45.507500  48.647499  45.283626  43.188022   \n",
       "17  48.950001  49.897499  48.112499  49.224998  46.409733  44.116788   \n",
       "18  49.445000  50.467499  47.792500  50.465000  47.568381  45.093436   \n",
       "19  50.465000  52.492500  50.305000  52.172501  48.883843  46.182523   \n",
       "20  51.799999  54.667500  50.092499  54.035000  50.355602  47.390596   \n",
       "21  54.035000  54.292500  51.950001  52.840000  51.065430  48.228966   \n",
       "22  52.997501  54.250000  52.625000  54.240002  51.972451  49.153741   \n",
       "23  54.327499  54.340000  47.807499  49.419998  51.243179  49.194703   \n",
       "24  50.012501  50.075001  45.145000  48.375000  36.048206  33.620895   \n",
       "25  38.790001  39.150002  35.875000  36.139999  36.048206  33.620895   \n",
       "26  36.262501  36.875000  34.645000  36.689999  36.231575  34.093064   \n",
       "27  37.435001  41.575001  37.169998  41.237499  37.661839  35.192208   \n",
       "28  41.582500  42.325001  40.325001  42.025002  38.908457  36.243407   \n",
       "29  42.097500  42.482498  39.389999  41.097500  39.533898  36.990191   \n",
       "30  41.235001  42.517502  40.154999  41.802502  40.182070  37.730546   \n",
       "31  42.097500  43.639999  38.227501  38.990002  39.841479  37.924309   \n",
       "32  39.917500  42.417500  39.799999  40.375000  39.993914  38.301338   \n",
       "33  40.622501  41.820000  39.342499  40.952499  40.267795  38.709209   \n",
       "34  41.145000  42.974998  40.564999  42.615002  40.938426  39.310100   \n",
       "35  42.095001  42.145000  40.677502  40.922501  40.933876  39.558162   \n",
       "36  41.384998  45.027500  41.330002  45.027500  42.103483  40.399598   \n",
       "37  46.285000  47.799999  44.402500  44.750000  42.859630  41.068891   \n",
       "38  44.437500  44.932499  42.540001  44.692501  43.383308  41.626369   \n",
       "39  45.200001  45.500000  44.250000  45.325001  43.938077  42.195390   \n",
       "40  45.697498  48.750000  45.507500  48.647499  45.283626  43.188022   \n",
       "41  48.950001  49.897499  48.112499  49.224998  46.409733  44.116788   \n",
       "42  49.445000  50.467499  47.792500  50.465000  47.568381  45.093436   \n",
       "43  50.465000  52.492500  50.305000  52.172501  48.883843  46.182523   \n",
       "44  51.799999  54.667500  50.092499  54.035000  50.355602  47.390596   \n",
       "45  54.035000  54.292500  51.950001  52.840000  51.065430  48.228966   \n",
       "46  52.997501  54.250000  52.625000  54.240002  51.972451  49.153741   \n",
       "47  54.327499  54.340000  47.807499  49.419998  51.243179  49.194703   \n",
       "48  50.012501  50.075001  45.145000  47.872501  50.280128  48.991288   \n",
       "49  48.014999  54.512501  46.150002  53.849998  36.231575  34.093064   \n",
       "\n",
       "        EMA24     labels prediction    Datetime    profit  \n",
       "In                                                         \n",
       "0   29.691369         nn         nn  2017-06-19 -1.637501  \n",
       "1   30.207260         nn         nn  2017-06-26 -1.637501  \n",
       "2   30.725879         nn         nn  2017-07-03 -1.637501  \n",
       "3   31.566809         nn         nn  2017-07-10 -1.637501  \n",
       "4   32.403464         nn         nn  2017-07-17 -1.637501  \n",
       "5   33.098987         nn         nn  2017-07-24 -1.637501  \n",
       "6   33.795268         nn         nn  2017-07-31 -1.637501  \n",
       "7   34.210847         nn         nn  2017-08-07 -1.637501  \n",
       "8   34.703979         nn         nn  2017-08-14 -1.637501  \n",
       "9   35.203861         nn         nn  2017-08-21 -1.637501  \n",
       "10  35.796752         nn         nn  2017-08-28 -1.637501  \n",
       "11  36.206812         nn         nn  2017-09-04 -1.637501  \n",
       "12  36.912467         nn         nn  2017-09-11 -1.637501  \n",
       "13  37.539470         nn         nn  2017-09-18 -1.637501  \n",
       "14  38.111712         nn         nn  2017-09-25 -1.637501  \n",
       "15  38.688775         nn         nn  2017-10-02 -1.637501  \n",
       "16  39.485473         nn         nn  2017-10-09 -1.637501  \n",
       "17  40.264635         nn         nn  2017-10-16 -1.637501  \n",
       "18  41.080664         nn         nn  2017-10-23 -1.637501  \n",
       "19  41.968011         nn         nn  2017-10-30 -1.637501  \n",
       "20  42.933370         nn         nn  2017-11-06 -1.637501  \n",
       "21  43.725901         nn         nn  2017-11-13 -1.637501  \n",
       "22  44.567029         nn         nn  2017-11-20 -1.637501  \n",
       "23  44.955266         nn         nn  2017-11-27 -1.637501  \n",
       "24  30.207260  50.075001  52.733866  2017-11-28 -1.637501  \n",
       "25  30.207260         nn         nn  2017-06-26  3.421984  \n",
       "26  30.725879         nn         nn  2017-07-03  3.421984  \n",
       "27  31.566809         nn         nn  2017-07-10  3.421984  \n",
       "28  32.403464         nn         nn  2017-07-17  3.421984  \n",
       "29  33.098987         nn         nn  2017-07-24  3.421984  \n",
       "30  33.795268         nn         nn  2017-07-31  3.421984  \n",
       "31  34.210847         nn         nn  2017-08-07  3.421984  \n",
       "32  34.703979         nn         nn  2017-08-14  3.421984  \n",
       "33  35.203861         nn         nn  2017-08-21  3.421984  \n",
       "34  35.796752         nn         nn  2017-08-28  3.421984  \n",
       "35  36.206812         nn         nn  2017-09-04  3.421984  \n",
       "36  36.912467         nn         nn  2017-09-11  3.421984  \n",
       "37  37.539470         nn         nn  2017-09-18  3.421984  \n",
       "38  38.111712         nn         nn  2017-09-25  3.421984  \n",
       "39  38.688775         nn         nn  2017-10-02  3.421984  \n",
       "40  39.485473         nn         nn  2017-10-09  3.421984  \n",
       "41  40.264635         nn         nn  2017-10-16  3.421984  \n",
       "42  41.080664         nn         nn  2017-10-23  3.421984  \n",
       "43  41.968011         nn         nn  2017-10-30  3.421984  \n",
       "44  42.933370         nn         nn  2017-11-06  3.421984  \n",
       "45  43.725901         nn         nn  2017-11-13  3.421984  \n",
       "46  44.567029         nn         nn  2017-11-20  3.421984  \n",
       "47  44.955266         nn         nn  2017-11-27  3.421984  \n",
       "48  45.188645         nn         nn  2017-12-04  3.421984  \n",
       "49  30.725879  54.512501  51.436983  2017-12-05  3.421984  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('04_stockprediction': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ae9b00e5548af57dc5d4c583df0ad518b3d501960f1be5c69ca4a560e00ae05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
